{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from skimage import color\n",
    "import gc\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# for training with HSV channels\n",
    "def rgb2hsv(img):\n",
    "    # img - image tensor\n",
    "    img_original = img.numpy().transpose((1, 2, 0))\n",
    "    img_hsv = color.rgb2hsv(img_original)\n",
    "    img_hs = img_hsv[:, :, 0:2]\n",
    "    img_hs = torch.from_numpy(img_hs.transpose((2, 0, 1))).float()\n",
    "    img_v = img_hsv[:, :, 2]\n",
    "    img_v = torch.from_numpy(img_v).float().unsqueeze(0)\n",
    "    img_gray = color.rgb2gray(img_original)\n",
    "    img_gray = torch.from_numpy(img_gray).unsqueeze(0).float()\n",
    "    return img_gray, img_hs, img_v\n",
    "\n",
    "def hsv2rgb(v, h, s):\n",
    "    # l, a, b - image tensors representing LAB channels\n",
    "    img_hsv = torch.stack([h, s, v])\n",
    "    img_rgb = color.hsv2rgb(img_hsv.cpu().detach().numpy().transpose((1, 2, 0)))\n",
    "    img_rgb = torch.from_numpy(img_rgb.transpose((2, 0, 1))).float()\n",
    "    return img_rgb\n",
    "\n",
    "# for training with LAB channels\n",
    "\n",
    "def rgb2lab(img):\n",
    "    # img - image tensor\n",
    "    img_original = img.numpy().transpose((1, 2, 0))\n",
    "    img_lab = color.rgb2lab(img_original)\n",
    "    img_lab = (img_lab + 128) / 255\n",
    "    img_ab = img_lab[:, :, 1:3]\n",
    "    img_ab = torch.from_numpy(img_ab.transpose((2, 0, 1))).float()\n",
    "    img_l = img_lab[:, :, 0]\n",
    "    img_l = torch.from_numpy(img_l).float().unsqueeze(0)\n",
    "    img_gray = color.rgb2gray(img_original)\n",
    "    img_gray = torch.from_numpy(img_gray).unsqueeze(0).float()\n",
    "    return img_gray, img_ab, img_l\n",
    "\n",
    "def lab2rgb(l, a, b):\n",
    "    # l, a, b - image tensors representing LAB channels\n",
    "    img_lab = torch.stack([l, a, b])\n",
    "    img_lab = img_lab * 255 - 128\n",
    "    img_rgb = color.lab2rgb(img_lab.cpu().detach().numpy().transpose((1, 2, 0)))\n",
    "    img_rgb = torch.from_numpy(img_rgb.transpose((2, 0, 1))).float()\n",
    "    return img_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global feature extractor based on Resnet152\n",
    "class ExtractorResnet152(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExtractorResnet152, self).__init__()\n",
    "        model = torchvision.models.resnet152()\n",
    "        model.conv1.weight = nn.Parameter(model.conv1.weight.sum(dim=1).unsqueeze(1))\n",
    "        self.layers = nn.Sequential(\n",
    "            *list(model.children())[0:6]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "# auto encoder decoder based on Resnet18\n",
    "class ColorNetResnet18(nn.Module):\n",
    "\n",
    "    def __init__(self, mid_input_size=0):\n",
    "        super(ColorNetResnet18, self).__init__()\n",
    "        self.mid_input_size = mid_input_size\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        resnet.conv1.weight = nn.Parameter(resnet.conv1.weight.sum(dim=1).unsqueeze(1))\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[0:6])\n",
    "        self.decoder = nn.Sequential(     \n",
    "            nn.Conv2d(mid_input_size + 128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, in_=None):\n",
    "        x = self.encoder(x)\n",
    "        if self.mid_input_size > 0:\n",
    "            x = torch.cat([x, in_], dim=1)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColorNetResnet18(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Conv2d(640, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (4): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU()\n",
       "    (10): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (11): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU()\n",
       "    (14): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): Upsample(scale_factor=2.0, mode=nearest)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {\n",
    "    \"resnet18\": ColorNetResnet18\n",
    "}\n",
    "extractors = {\n",
    "    \"resnet152\": ExtractorResnet152\n",
    "}\n",
    "model_name = \"resnet18\"\n",
    "extractor = ExtractorResnet152()\n",
    "model = models[model_name](mid_input_size=extractor(torch.zeros((1, 1, 256, 256))).shape[1])\n",
    "model.load_state_dict(torch.load('models/resnet18_2022-05-22_13:40:31:218825.pth'))\n",
    "extractor.to(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(dataset_path):\n",
    "    dataset = torchvision.datasets.ImageFolder(dataset_path, transform=T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.ToTensor()\n",
    "    ]))\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    return torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "lhq256_train, lhq256_test = get_datasets(\"LHQ256\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = str(datetime.datetime.now()).replace('.', ':').replace(' ', '_')\n",
    "checkpoint_path = f'models/{model_name}_{time}.pth'\n",
    "\n",
    "def train(model, extractor, train_dataset, plot_name, convert_fn, num_batches=10, batch_size=16, epochs=100, lr=0.1, decay=1e-10):\n",
    "  losses = []\n",
    "  criterion = nn.MSELoss().to(device)\n",
    "  optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=decay)\n",
    "  min_loss = 10000\n",
    "  with tqdm(range(epochs)) as eps:\n",
    "    total_loss = 0\n",
    "    for e in eps:\n",
    "      # train for num_batches batches per epoch\n",
    "      try:\n",
    "        # sample an image batch\n",
    "        batch_idx = random.choices(np.arange(len(train_dataset)), k=batch_size)\n",
    "        train_images = [convert_fn(train_dataset[n][0]) for n in batch_idx]\n",
    "        \n",
    "        gray_inputs = torch.stack([im[0] for im in train_images]).to(device)\n",
    "        color_inputs = torch.stack([im[1] for im in train_images]).to(device)\n",
    "      except:\n",
    "        continue\n",
    "      \n",
    "      # backpropagate and step\n",
    "      optimizer.zero_grad()\n",
    "      global_features = extractor(gray_inputs).to(device)\n",
    "      color_outputs = model(gray_inputs, global_features).to(device)\n",
    "      loss = criterion(color_outputs, color_inputs)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # clear memory\n",
    "      del gray_inputs\n",
    "      del color_inputs\n",
    "      del global_features\n",
    "      del color_outputs\n",
    "      gc.collect()\n",
    "      torch.cuda.empty_cache()\n",
    "      total_loss += loss.item().real\n",
    "\n",
    "      # plot losses\n",
    "      if (e % num_batches == 0 and e != 0):\n",
    "        # mean loss per epoch\n",
    "        mean_loss = total_loss / num_batches\n",
    "        eps.set_postfix(loss=mean_loss)\n",
    "\n",
    "        # save model if perform better\n",
    "        if (mean_loss < min_loss):\n",
    "          min_loss = mean_loss\n",
    "          torch.save(model.state_dict(), checkpoint_path)\n",
    "        losses.append(mean_loss)\n",
    "        total_loss = 0\n",
    "        plt.figure()\n",
    "        plt.plot(losses)\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel(\"iterations\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.savefig(plot_name)\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 517/2000 [02:19<10:28,  2.36it/s]"
     ]
    }
   ],
   "source": [
    "train(model, extractor, lhq256_train, convert_fn=rgb2lab, num_batches=5, batch_size=16, epochs=2000, lr=0.01, plot_name=f\"plots/lab_losses_{time}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 5.94 GiB total capacity; 815.35 MiB already allocated; 15.75 MiB free; 832.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/homes/iws/lpremc/cse455/image_colorization/image_color_.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bboromir.cs.washington.edu/homes/iws/lpremc/cse455/image_colorization/image_color_.ipynb#ch0000006vscode-remote?line=8'>9</a>\u001b[0m color_inputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([im[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m im \u001b[39min\u001b[39;00m test_images])\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboromir.cs.washington.edu/homes/iws/lpremc/cse455/image_colorization/image_color_.ipynb#ch0000006vscode-remote?line=9'>10</a>\u001b[0m l_inputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([im[\u001b[39m2\u001b[39m] \u001b[39mfor\u001b[39;00m im \u001b[39min\u001b[39;00m test_images])\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bboromir.cs.washington.edu/homes/iws/lpremc/cse455/image_colorization/image_color_.ipynb#ch0000006vscode-remote?line=10'>11</a>\u001b[0m global_features \u001b[39m=\u001b[39m extractor(gray_inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboromir.cs.washington.edu/homes/iws/lpremc/cse455/image_colorization/image_color_.ipynb#ch0000006vscode-remote?line=11'>12</a>\u001b[0m color_outputs \u001b[39m=\u001b[39m model(gray_inputs, global_features)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboromir.cs.washington.edu/homes/iws/lpremc/cse455/image_colorization/image_color_.ipynb#ch0000006vscode-remote?line=12'>13</a>\u001b[0m rgb_inputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([dataset[n][\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m batch_idx])\n",
      "File \u001b[0;32m~/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/homes/iws/lpremc/cse455/image_colorization/image_color_.ipynb Cell 2'\u001b[0m in \u001b[0;36mExtractorResnet152.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboromir.cs.washington.edu/homes/iws/lpremc/cse455/image_colorization/image_color_.ipynb#ch0000001vscode-remote?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bboromir.cs.washington.edu/homes/iws/lpremc/cse455/image_colorization/image_color_.ipynb#ch0000001vscode-remote?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bboromir.cs.washington.edu/homes/iws/lpremc/cse455/image_colorization/image_color_.ipynb#ch0000001vscode-remote?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=445'>446</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=446'>447</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=438'>439</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=439'>440</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=440'>441</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=441'>442</a>\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=442'>443</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    <a href='file:///homes/iws/lpremc/miniconda3/envs/cse455/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=443'>444</a>\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 5.94 GiB total capacity; 815.35 MiB already allocated; 15.75 MiB free; 832.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# try predict colors\n",
    "pil_image = T.ToPILImage()\n",
    "convert_fn = rgb2lab\n",
    "revert_fn = lab2rgb\n",
    "dataset = lhq256_test\n",
    "batch_idx = random.choices(range(len(dataset)), k=4)\n",
    "test_images = [convert_fn(dataset[n][0]) for n in batch_idx]\n",
    "gray_inputs = torch.stack([im[0] for im in test_images]).to(device)\n",
    "color_inputs = torch.stack([im[1] for im in test_images]).to(device)\n",
    "l_inputs = torch.stack([im[2] for im in test_images]).to(device)\n",
    "global_features = extractor(gray_inputs)\n",
    "color_outputs = model(gray_inputs, global_features)\n",
    "rgb_inputs = torch.stack([dataset[n][0] for n in batch_idx])\n",
    "rgb_outputs = torch.stack([revert_fn(l_inputs[i][0], color_outputs[i][0], color_outputs[i][1]) for i in range(len(color_outputs))])\n",
    "plt.figure(figsize=(12, 35))\n",
    "nrows = len(rgb_outputs)\n",
    "ncols = 4\n",
    "for i in range(nrows):\n",
    "    plt.subplot(nrows, ncols, i*ncols+1)\n",
    "    plt.imshow(pil_image(rgb_inputs[i]))\n",
    "    plt.subplot(nrows, ncols, i*ncols+2)\n",
    "    plt.imshow(pil_image(rgb_outputs[i]))\n",
    "    plt.subplot(nrows, ncols, i*ncols+3)\n",
    "    plt.imshow(pil_image(rgb_inputs[i]))\n",
    "    plt.subplot(nrows, ncols, i*ncols+4)\n",
    "    plt.imshow(pil_image(rgb_outputs[i]))\n",
    "plt.savefig(\"result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "770b06b1d0396d73f0d7642da8c4cbc38ebd21643ce668636937fb4f23fbfecf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('cse455': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
